{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing GPT-2 124M with JAX, FLAX, and NNX\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this Jupyter notebook, we will implement the GPT-2 124M (small) model, a transformer-based language model developed by OpenAI, using JAX and FLAX with the NNX API. Our goal is to:\n",
        "\n",
        "- Build the GPT-2 model architecture from scratch using FLAX's NNX API.\n",
        "- Load pre-trained weights from Hugging Face's GPT-2 124M model.\n",
        "- Optimize the model with JAX's JIT compilation.\n",
        "- Demonstrate its usage by computing logits for a sample input.\n",
        "\n",
        "This notebook serves as both a practical guide and an educational resource, walking you through each step with clear code and explanations. We'll assume some familiarity with Python and machine learning, but we'll explain JAX, FLAX, and transformer concepts as needed.\n",
        "\n",
        "Let's get started!"
      ],
      "metadata": {
        "id": "8bpWJvyINzCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "First, we need to install the necessary libraries and import the required modules.\n",
        "\n",
        "### Install Libraries\n",
        "\n",
        "Run the following commands in your terminal or a notebook cell to install the dependencies. You may need to adjust the JAX installation based on your hardware (CPU, GPU, or TPU).\n"
      ],
      "metadata": {
        "id": "A4DRORlqN37u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L8H7XZ-yxkr",
        "outputId": "3e30d649-6ff3-406c-938b-69ff719fa9db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (0.5.1)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.11/dist-packages (0.10.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from jax) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax) (1.14.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax) (1.1.0)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (from flax) (0.2.4)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax) (0.11.10)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax) (0.1.73)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.11/dist-packages (from flax) (14.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from flax) (4.13.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from flax) (0.1.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax) (2.19.1)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from optax->flax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.11/dist-packages (from optax->flax) (0.1.89)\n",
            "Requirement already satisfied: etils[epy] in /usr/local/lib/python3.11/dist-packages (from optax->flax) (1.12.2)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax) (5.29.4)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax) (4.12.2)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax) (3.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.87->optax->flax) (1.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.21.0)\n",
            "Looking in links: https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
            "Requirement already satisfied: jax[tpu] in /usr/local/lib/python3.11/dist-packages (0.5.2)\n",
            "Requirement already satisfied: jaxlib<=0.5.2,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from jax[tpu]) (0.5.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax[tpu]) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from jax[tpu]) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax[tpu]) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax[tpu]) (1.14.1)\n",
            "Collecting libtpu==0.0.10.* (from jax[tpu])\n",
            "  Downloading libtpu-0.0.10.1-py3-none-manylinux_2_27_x86_64.whl.metadata (202 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from jax[tpu]) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->jax[tpu]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->jax[tpu]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->jax[tpu]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->jax[tpu]) (2025.1.31)\n",
            "Downloading libtpu-0.0.10.1-py3-none-manylinux_2_27_x86_64.whl (129.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.7/129.7 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libtpu\n",
            "  Attempting uninstall: libtpu\n",
            "    Found existing installation: libtpu 0.0.7.1\n",
            "    Uninstalling libtpu-0.0.7.1:\n",
            "      Successfully uninstalled libtpu-0.0.7.1\n",
            "Successfully installed libtpu-0.0.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install jax jaxlib flax transformers\n",
        "!pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Modules\n",
        "\n",
        "Now, let's import the modules we'll use throughout the notebook.\n",
        "- **JAX**: Provides high-performance numerical computing and automatic differentiation.\n",
        "- **FLAX**: A neural network library built on JAX, with NNX being a modern API for building models.\n",
        "- **Transformers**: Hugging Face's library for accessing pre-trained models and tokenizers.\n",
        "- **NumPy and Torch**: Used for weight conversions between PyTorch (Hugging Face) and JAX.\n"
      ],
      "metadata": {
        "id": "BvqhhJd-N94d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VQt713Ojy0E4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd172101-3954-48d2-9189-de4b01c7d969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "print(jax.devices())\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "from transformers import GPT2Config, GPT2Tokenizer, GPT2Model as HF_GPT2Model\n",
        "from transformers import GPT2LMHeadModel\n",
        "import torch\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Model Architecture\n",
        "\n",
        "Here, we'll define the GPT-2 model architecture using FLAX's NNX API, ensuring it matches Hugging Face's implementation. GPT-2 is a decoder-only transformer with the following components:\n",
        "\n",
        "- **Token and Positional Embeddings**: Convert input tokens and their positions into dense vectors.\n",
        "- **Transformer Blocks**: A stack of layers, each containing multi-head self-attention and a feed-forward network, both with layer normalization and residual connections.\n",
        "- **Final Layer Normalization**: Normalizes the output of the last transformer block.\n",
        "\n",
        "We'll break this down into modular components.\n"
      ],
      "metadata": {
        "id": "cW9HZpY4ONuP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Components\n",
        "\n",
        "#### Multi-Head Attention\n",
        "\n",
        "The multi-head attention mechanism allows the model to focus on different parts of the input sequence. GPT-2 uses causal (masked) attention to ensure that each token only attends to previous tokens.\n",
        "\n",
        "\n",
        "**Notes**:\n",
        "- `c_attn` projects the input to query (Q), key (K), and value (V) vectors in one go.\n",
        "- The causal mask (`jnp.tril`) ensures attention is only applied to previous positions.\n",
        "- Dropout is included for regularization, disabled during inference with `deterministic=True`.\n",
        "\n"
      ],
      "metadata": {
        "id": "F1y8NxFGOTmp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lBkJ058VzL25"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nnx.Module):\n",
        "    def __init__(self, n_embd: int, n_head: int, attn_pdrop: float, resid_pdrop: float, *, rngs: nnx.Rngs):\n",
        "        self.c_attn = nnx.Linear(n_embd, 3 * n_embd, rngs=rngs)  # Combined Q, K, V projection\n",
        "        self.c_proj = nnx.Linear(n_embd, n_embd, rngs=rngs)      # Output projection\n",
        "        self.attn_dropout = nnx.Dropout(attn_pdrop, rngs=rngs)\n",
        "        self.resid_dropout = nnx.Dropout(resid_pdrop, rngs=rngs)\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "\n",
        "    def __call__(self, x: jnp.ndarray, rngs: nnx.Rngs, deterministic: bool = False):\n",
        "        batch, seq_len, _ = x.shape\n",
        "        head_dim = self.n_embd // self.n_head\n",
        "\n",
        "        # Compute Q, K, V in one linear transformation and split\n",
        "        qkv = self.c_attn(x)  # (batch, seq_len, 3 * n_embd)\n",
        "        q, k, v = jnp.split(qkv, 3, axis=-1)  # Each (batch, seq_len, n_embd)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.reshape(batch, seq_len, self.n_head, head_dim).transpose(0, 2, 1, 3)\n",
        "        k = k.reshape(batch, seq_len, self.n_head, head_dim).transpose(0, 2, 1, 3)\n",
        "        v = v.reshape(batch, seq_len, self.n_head, head_dim).transpose(0, 2, 1, 3)\n",
        "\n",
        "        # Attention scores with causal masking\n",
        "        scores = jnp.matmul(q, k.transpose(0, 1, 3, 2)) / jnp.sqrt(head_dim)\n",
        "        mask = jnp.tril(jnp.ones((seq_len, seq_len), dtype=bool))\n",
        "        scores = jnp.where(mask, scores, -1e9)\n",
        "        attn_weights = jax.nn.softmax(scores, axis=-1)\n",
        "        attn_weights = self.attn_dropout(attn_weights, rngs=rngs, deterministic=deterministic)\n",
        "\n",
        "        # Weighted sum of values\n",
        "        out = jnp.matmul(attn_weights, v)\n",
        "        out = out.transpose(0, 2, 1, 3).reshape(batch, seq_len, self.n_embd)\n",
        "\n",
        "        # Output projection\n",
        "        out = self.c_proj(out)\n",
        "        out = self.resid_dropout(out, rngs=rngs, deterministic=deterministic)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Feed-Forward Network\n",
        "\n",
        "The feed-forward network (FFN) applies two linear transformations with a GELU activation in between.\n",
        "\n",
        "\n",
        "**Notes**:\n",
        "- `intermediate_size` is typically 4 * `n_embd` in GPT-2 (e.g., 3072 for `n_embd=768`).\n",
        "- GELU activation adds nonlinearity, followed by dropout.\n"
      ],
      "metadata": {
        "id": "_V9H1t3WOiqi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0Ig2l8nMzPF3"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nnx.Module):\n",
        "    def __init__(self, n_embd: int, intermediate_size: int, resid_pdrop: float, *, rngs: nnx.Rngs):\n",
        "        self.c_fc = nnx.Linear(n_embd, intermediate_size, rngs=rngs)  # Expansion\n",
        "        self.c_proj = nnx.Linear(intermediate_size, n_embd, rngs=rngs)  # Projection back\n",
        "        self.act = nnx.gelu\n",
        "        self.dropout = nnx.Dropout(resid_pdrop, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x: jnp.ndarray, rngs: nnx.Rngs, deterministic: bool = False):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.act(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x, rngs=rngs, deterministic=deterministic)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformer Block\n",
        "\n",
        "Each transformer block combines multi-head attention and a feed-forward network with residual connections and layer normalization.\n",
        "\n",
        "\n",
        "\n",
        "**Notes**:\n",
        "- Layer normalization is applied before attention and FFN (pre-LN transformer design).\n",
        "- Residual connections help with gradient flow during training.\n"
      ],
      "metadata": {
        "id": "u2joNdpuOoZu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "08LQwedszSqx"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nnx.Module):\n",
        "    def __init__(self, n_embd: int, n_head: int, intermediate_size: int, attn_pdrop: float, resid_pdrop: float, *, rngs: nnx.Rngs):\n",
        "        self.ln_1 = nnx.LayerNorm(n_embd, rngs=rngs)  # Pass rngs to LayerNorm\n",
        "        self.attn = MultiHeadAttention(n_embd, n_head, attn_pdrop, resid_pdrop, rngs=rngs)\n",
        "        self.ln_2 = nnx.LayerNorm(n_embd, rngs=rngs)  # Pass rngs to LayerNorm\n",
        "        self.mlp = FeedForward(n_embd, intermediate_size, resid_pdrop, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x: jnp.ndarray, rngs: nnx.Rngs, deterministic: bool = False):\n",
        "        residual = x\n",
        "        x = self.ln_1(x)\n",
        "        x = self.attn(x, rngs, deterministic)\n",
        "        x = x + residual  # Residual connection\n",
        "        residual = x\n",
        "        x = self.ln_2(x)\n",
        "        x = self.mlp(x, rngs, deterministic)\n",
        "        x = x + residual  # Residual connection\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### GPT-2 Model\n",
        "\n",
        "\n",
        "\n",
        "**Architecture Visualization** (Text-based):\n",
        "\n",
        "```\n",
        "Input IDs (batch, seq_len)\n",
        "  ↓\n",
        "[Token Embeddings] + [Positional Embeddings] → [Dropout]\n",
        "  ↓\n",
        "[Transformer Block 1] → [Transformer Block 2] → ... → [Transformer Block N]\n",
        "  ↓\n",
        "[Final LayerNorm]\n",
        "  ↓\n",
        "Hidden States (batch, seq_len, n_embd)\n",
        "```\n",
        "\n",
        "Where each `Transformer Block` is:\n",
        "\n",
        "```\n",
        "Input → [LayerNorm 1] → [Multi-Head Attention] → [Add Residual] → [LayerNorm 2] → [Feed-Forward] → [Add Residual] → Output\n",
        "```\n",
        "\n",
        "**Notes**:\n",
        "- The model matches Hugging Face's `GPT2Model`, outputting hidden states (not logits).\n",
        "- Config parameters (e.g., `n_embd`, `n_layer`) are sourced from `GPT2Config`.\n"
      ],
      "metadata": {
        "id": "eLNpg95DOuwG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DC7LuC1QzUhf"
      },
      "outputs": [],
      "source": [
        "class GPT2Model(nnx.Module):\n",
        "    def __init__(self, config: GPT2Config, *, rngs: nnx.Rngs):\n",
        "        self.wte = nnx.Embed(config.vocab_size, config.n_embd, rngs=rngs)\n",
        "        self.wpe = nnx.Embed(config.n_positions, config.n_embd, rngs=rngs)\n",
        "        self.drop = nnx.Dropout(config.embd_pdrop, rngs=rngs)\n",
        "        self.h = [\n",
        "            TransformerBlock(\n",
        "                n_embd=config.n_embd,\n",
        "                n_head=config.n_head,\n",
        "                intermediate_size=config.n_inner or 4 * config.n_embd,\n",
        "                attn_pdrop=config.attn_pdrop,\n",
        "                resid_pdrop=config.resid_pdrop,\n",
        "                rngs=rngs\n",
        "            ) for _ in range(config.n_layer)\n",
        "        ]\n",
        "        self.ln_f = nnx.LayerNorm(config.n_embd, epsilon=1e-5, rngs=rngs)\n",
        "        self.lm_head = nnx.Linear(config.n_embd, config.vocab_size, use_bias=False, rngs=rngs)\n",
        "\n",
        "    def __call__(self, input_ids: jnp.ndarray, rngs: nnx.Rngs, deterministic: bool = False):\n",
        "\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        position_ids = jnp.arange(seq_len)  # Shape: (seq_len,)\n",
        "\n",
        "        # Token embeddings\n",
        "        token_emb = self.wte(input_ids)  # Shape: (batch_size, seq_len, n_embd)\n",
        "\n",
        "        # Positional embeddings\n",
        "        position_emb = self.wpe(position_ids)  # Shape: (seq_len, n_embd)\n",
        "\n",
        "        # Ensure position_emb is broadcasted to match token_emb\n",
        "        position_emb = position_emb[None, :, :]  # Add batch dimension: (1, seq_len, n_embd)\n",
        "\n",
        "        # Combine embeddings\n",
        "        x = token_emb + position_emb  # Shape: (batch_size, seq_len, n_embd)\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.drop(x, rngs=rngs, deterministic=deterministic)\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for i, block in enumerate(self.h):\n",
        "            x = block(x, rngs, deterministic)\n",
        "\n",
        "        # Final LayerNorm\n",
        "        x = self.ln_f(x)  # Shape should still be (batch_size, seq_len, n_embd)\n",
        "\n",
        "        # Apply lm_head\n",
        "        logits = self.lm_head(x)  # Shape: (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Pre-trained Weights\n",
        "\n",
        "We'll load the pre-trained weights from Hugging Face's GPT-2 124M model and map them to our NNX model.\n",
        "\n",
        "### Load Hugging Face Model and Config\n",
        "\n"
      ],
      "metadata": {
        "id": "sHoB2791O57o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9YtQHdsyzWWB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "992dae584a224f1a9678e1dd7c723753",
            "999e4ed9a43842ce8e8bc6ed1cf0afa6",
            "c01281c6cb0043bfba6227061a7a7d10",
            "7f80926b1b854f32be61d6c0087b2e8c",
            "caa08e69fcf049c0a55b8ff040c9d2ca",
            "c75f1c301f5441c8b20dcfbc53c94f6a",
            "350f3d47b8af478e96aaaed4645e3738",
            "78b60bea0eb845afad387897c8653ee4",
            "5d792b4752da47ba9cc5d6c80b8c6ead",
            "3fb9ef51669f4a88871a3e1fe8af506d",
            "a0ec79f3fa9b4fa487350a5903446c20",
            "4dcba774df3948f1a2d480088f97a864",
            "408d80a0ef8f492597d8a5c8082607db",
            "8688e0ec5ef9490d81eb551715d3d5d8",
            "59adf9c199c94095a6db177906b9dd6b",
            "3cd3c62e9fb94a0ba659a874fee530f2",
            "a3ba0b14c8bc4cd7a159c59685aa478b",
            "e8246608c2714657a610fea8ecc7d7f0",
            "35a0ddefcad74cc2bd7ebd38179d1556",
            "c0a10a02c3a94b94bf41ab39c9ec4573",
            "e91d1bf3d70d4f0c833ea6e623d7cf90",
            "7459739f444a4af09c663b167d1188f2"
          ]
        },
        "outputId": "99d00eb0-ac7c-4010-b02b-afa0b1b00c21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "992dae584a224f1a9678e1dd7c723753"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4dcba774df3948f1a2d480088f97a864"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load configuration and pre-trained model\n",
        "config = GPT2Config.from_pretrained(\"gpt2\")  # GPT-2 small (124M parameters)\n",
        "hf_model = HF_GPT2Model.from_pretrained(\"gpt2\")\n",
        "hf_state_dict = hf_model.state_dict()  # PyTorch state dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize NNX Model\n"
      ],
      "metadata": {
        "id": "D2eOzU-vO-3Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_4z8ffH8zZAf"
      },
      "outputs": [],
      "source": [
        "# Initialize NNX model with RNGs\n",
        "rngs = nnx.Rngs(0)  # Seed 0, arbitrary since weights are overwritten\n",
        "model = GPT2Model(config, rngs=rngs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Map Weights\n",
        "\n",
        "Hugging Face uses PyTorch's conventions, where linear weights are `(out_features, in_features)`, while FLAX uses `(in_features, out_features)`. Thus, we need to transpose linear weights. We'll also convert PyTorch tensors to JAX arrays.\n",
        "\n",
        "\n",
        "\n",
        "**Pitfalls to Watch**:\n",
        "- **Transposition**: Forgetting to transpose linear weights (`c_attn`, `c_proj`, `c_fc`) will mismatch dimensions.\n",
        "- **Naming**: Ensure parameter names align exactly with Hugging Face's state dict keys.\n",
        "- **Dropout**: Dropout layers have no weights, so they don’t need loading.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6M_TMrDcPcLa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "V0MSF-HBzgBV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "c38b0b4cebc74037856858e41cc642b5",
            "697bc29630c442f5a735ae25a4199524",
            "d74d190c7a5145ce963814cad271ce42",
            "8e21a1536abe42a780fc49009100bf96",
            "226621c79d5d4ff5aa0fcfdae39d0351",
            "0d381688dd114e79860393ed0c92f1f5",
            "354901bed384451983429681b9891c01",
            "fb844b459f82432889cb0aed03eceef8",
            "ba07e34dd1d34bd8bc368e7101d9b7de",
            "df6d571b2a234a328778ca95cf4a95e7",
            "b697278b59e94b70b7fe74335483ab9d"
          ]
        },
        "outputId": "ecfcb8f1-aa46-420f-8f79-c643c9dff8e9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c38b0b4cebc74037856858e41cc642b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face state dict keys: ['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias', 'lm_head.weight']\n",
            "lm_head weight found and loading...\n",
            "Original lm_head weight shape: (50257, 768)\n",
            "Final lm_head weight shape: (768, 50257)\n"
          ]
        }
      ],
      "source": [
        "config = GPT2Config.from_pretrained(\"gpt2\")\n",
        "hf_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")  # Use LMHeadModel to ensure lm_head weights are included\n",
        "hf_state_dict = hf_model.state_dict()\n",
        "\n",
        "rngs = nnx.Rngs(0)\n",
        "model = GPT2Model(config, rngs=rngs)\n",
        "\n",
        "print(\"Hugging Face state dict keys:\", list(hf_state_dict.keys()))\n",
        "\n",
        "def update_model_state(model, hf_state_dict):\n",
        "    state = nnx.state(model)\n",
        "\n",
        "    # Add 'transformer.' prefix to all keys\n",
        "    transformer_prefix = 'transformer.'\n",
        "\n",
        "    # Embeddings\n",
        "    state.wte.embedding = nnx.Variable(jnp.array(hf_state_dict[f'{transformer_prefix}wte.weight'].cpu().numpy()))\n",
        "    state.wpe.embedding = nnx.Variable(jnp.array(hf_state_dict[f'{transformer_prefix}wpe.weight'].cpu().numpy()))\n",
        "\n",
        "    # Transformer blocks\n",
        "    for i in range(config.n_layer):\n",
        "        prefix = f'{transformer_prefix}h.{i}'\n",
        "        state.h[i].ln_1.scale = nnx.Variable(jnp.array(hf_state_dict[f'{prefix}.ln_1.weight'].cpu().numpy()))\n",
        "        state.h[i].ln_1.bias = nnx.Variable(jnp.array(hf_state_dict[f'{prefix}.ln_1.bias'].cpu().numpy()))\n",
        "        state.h[i].ln_2.scale = nnx.Variable(jnp.array(hf_state_dict[f'{prefix}.ln_2.weight'].cpu().numpy()))\n",
        "        state.h[i].ln_2.bias = nnx.Variable(jnp.array(hf_state_dict[f'{prefix}.ln_2.bias'].cpu().numpy()))\n",
        "\n",
        "        # Attention weights\n",
        "        attn_weight = jnp.array(hf_state_dict[f'{prefix}.attn.c_attn.weight'].cpu().numpy())\n",
        "        if attn_weight.shape != (config.n_embd, 3 * config.n_embd):\n",
        "            attn_weight = attn_weight.T  # Transpose if needed to (n_embd, 3 * n_embd)\n",
        "        assert attn_weight.shape == (config.n_embd, 3 * config.n_embd), f\"Expected {(config.n_embd, 3 * config.n_embd)}, got {attn_weight.shape}\"\n",
        "        state.h[i].attn.c_attn.kernel = nnx.Variable(attn_weight)\n",
        "        state.h[i].attn.c_attn.bias = nnx.Variable(jnp.array(hf_state_dict[f'{prefix}.attn.c_attn.bias'].cpu().numpy()))\n",
        "\n",
        "        # c_proj weights\n",
        "        c_proj_weight = jnp.array(hf_state_dict[f'{prefix}.attn.c_proj.weight'].cpu().numpy())\n",
        "        if c_proj_weight.shape != (config.n_embd, config.n_embd):\n",
        "            c_proj_weight = c_proj_weight.T\n",
        "        state.h[i].attn.c_proj.kernel = nnx.Variable(c_proj_weight)\n",
        "        state.h[i].attn.c_proj.bias = nnx.Variable(jnp.array(hf_state_dict[f'{prefix}.attn.c_proj.bias'].cpu().numpy()))\n",
        "\n",
        "        # MLP weights\n",
        "        c_fc_weight = jnp.array(hf_state_dict[f'{prefix}.mlp.c_fc.weight'].cpu().numpy())\n",
        "        if c_fc_weight.shape != (config.n_embd, config.n_inner or 4 * config.n_embd):\n",
        "            c_fc_weight = c_fc_weight.T\n",
        "        state.h[i].mlp.c_fc.kernel = nnx.Variable(c_fc_weight)\n",
        "        state.h[i].mlp.c_fc.bias = nnx.Variable(jnp.array(hf_state_dict[f'{prefix}.mlp.c_fc.bias'].cpu().numpy()))\n",
        "\n",
        "        c_proj_weight = jnp.array(hf_state_dict[f'{prefix}.mlp.c_proj.weight'].cpu().numpy())\n",
        "        if c_proj_weight.shape != (config.n_inner or 4 * config.n_embd, config.n_embd):\n",
        "            c_proj_weight = c_proj_weight.T\n",
        "        state.h[i].mlp.c_proj.kernel = nnx.Variable(c_proj_weight)\n",
        "        state.h[i].mlp.c_proj.bias = nnx.Variable(jnp.array(hf_state_dict[f'{prefix}.mlp.c_proj.bias'].cpu().numpy()))\n",
        "\n",
        "    # Final LayerNorm\n",
        "    state.ln_f.scale = nnx.Variable(jnp.array(hf_state_dict[f'{transformer_prefix}ln_f.weight'].cpu().numpy()))\n",
        "    state.ln_f.bias = nnx.Variable(jnp.array(hf_state_dict[f'{transformer_prefix}ln_f.bias'].cpu().numpy()))\n",
        "\n",
        "    if 'lm_head.weight' in hf_state_dict:\n",
        "      print(\"lm_head weight found and loading...\")\n",
        "      # Get the lm_head weight and ensure it's in the right shape for nnx.Linear\n",
        "      lm_head_weight = jnp.array(hf_state_dict['lm_head.weight'].cpu().numpy())\n",
        "      print(f\"Original lm_head weight shape: {lm_head_weight.shape}\")\n",
        "\n",
        "      # The nnx.Linear expects kernel shape (n_embd, vocab_size)\n",
        "      if lm_head_weight.shape != (config.n_embd, config.vocab_size):\n",
        "          if lm_head_weight.shape == (config.vocab_size, config.n_embd):\n",
        "              lm_head_weight = lm_head_weight.T  # Transpose to (n_embd, vocab_size)\n",
        "          else:\n",
        "              print(f\"Unexpected lm_head weight shape: {lm_head_weight.shape}\")\n",
        "\n",
        "      print(f\"Final lm_head weight shape: {lm_head_weight.shape}\")\n",
        "      state.lm_head.kernel = nnx.Variable(lm_head_weight)\n",
        "    else:\n",
        "      # If lm_head.weight is not present, tie it to wte.embedding\n",
        "      # Make sure the shape is correct: (n_embd, vocab_size)\n",
        "      tied_weight = state.wte.embedding.value\n",
        "      if tied_weight.shape == (config.vocab_size, config.n_embd):\n",
        "          tied_weight = tied_weight.T  # Transpose to (n_embd, vocab_size)\n",
        "      state.lm_head.kernel = nnx.Variable(tied_weight)  # Tie weights\n",
        "      print(\"lm_head weight not found, tying to wte.embedding\")\n",
        "      print(f\"Tied lm_head weight shape: {state.lm_head.kernel.value.shape}\")\n",
        "\n",
        "    nnx.update(model, state)\n",
        "\n",
        "# Apply the update\n",
        "update_model_state(model, hf_state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## JIT Compilation\n",
        "\n",
        "JAX's JIT (Just-In-Time) compilation optimizes the model's forward pass for faster execution by compiling the computation graph.\n",
        "\n",
        "### Define and JIT the Forward Function\n",
        "\n",
        "\n",
        "\n",
        "### Why Use JIT?\n",
        "\n",
        "- **Performance**: JIT compiles the function into efficient machine code, leveraging XLA (Accelerated Linear Algebra) for hardware acceleration (e.g., GPU/TPU).\n",
        "- **Optimization**: It fuses operations, reducing overhead and improving speed, especially for repeated calls.\n",
        "\n",
        "**Note**: Since `model` is an NNX object with parameters, JIT treats it as a static argument, compiling the computation based on its structure and parameter shapes.\n"
      ],
      "metadata": {
        "id": "uYugTephPyEA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "76FB7ty-1fG3"
      },
      "outputs": [],
      "source": [
        "def forward(model, input_ids, rngs, deterministic):\n",
        "    return model(input_ids, rngs, deterministic)\n",
        "\n",
        "# JIT compile the forward function\n",
        "jit_forward = jax.jit(forward, static_argnums=(0,3))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from typing import Optional, Union, Dict, Any, List\n",
        "\n",
        "\n",
        "def generate(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt: str,\n",
        "    max_length: int = 50,\n",
        "    temperature: float = 0.3,\n",
        "    top_k: int = 5,\n",
        "    top_p: float = 0.95,\n",
        "    repetition_penalty: float =5,\n",
        "    eos_token_id: Optional[int] = None,\n",
        "    pad_token_id: Optional[int] = None,\n",
        "    do_sample: bool = True,\n",
        "    seed: int = 42,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generate text using a Flax GPT-2 model with JAX.\n",
        "\n",
        "    Args:\n",
        "        model: A FlaxGPT2LMHeadModel instance\n",
        "        tokenizer: A tokenizer from the transformers library\n",
        "        prompt: Initial text to condition generation\n",
        "        max_length: Maximum length of generated sequence (including prompt)\n",
        "        temperature: Controls randomness. Lower means more deterministic.\n",
        "        top_k: Number of highest probability tokens to consider for sampling\n",
        "        top_p: Cumulative probability threshold for nucleus sampling\n",
        "        repetition_penalty: Penalty for repeating tokens (1.0 = no penalty)\n",
        "        eos_token_id: Token ID that signals the end of generation\n",
        "        pad_token_id: Token ID for padding\n",
        "        do_sample: If False, use greedy decoding instead of sampling\n",
        "        seed: Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        Generated text as a string\n",
        "    \"\"\"\n",
        "    # Set default values for special tokens if not provided\n",
        "    if eos_token_id is None:\n",
        "        eos_token_id = tokenizer.eos_token_id\n",
        "    if pad_token_id is None:\n",
        "        pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "\n",
        "    # Encode the prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"np\")\n",
        "\n",
        "    # Create a JAX random key\n",
        "    key = jax.random.PRNGKey(seed)\n",
        "\n",
        "    input_ids = jax.device_put(input_ids)  # Move input to TPU\n",
        "    # Prepare inputs for the model\n",
        "    model_inputs = {\"input_ids\": input_ids}\n",
        "\n",
        "\n",
        "\n",
        "    # Create a function to calculate the next token probabilities\n",
        "    def get_next_token_logits(current_input_ids, key):\n",
        "        # Get logits from the model\n",
        "        rngs = nnx.Rngs(seed)\n",
        "        outputs = jit_forward(model,rngs=key,input_ids=current_input_ids,deterministic=True)\n",
        "        logits = outputs\n",
        "\n",
        "        # Get the logits for the last token\n",
        "        next_token_logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply repetition penalty\n",
        "        if repetition_penalty != 1.0:\n",
        "            # Create a mask for tokens that have appeared in the sequence\n",
        "            unique_tokens = jnp.unique(current_input_ids)\n",
        "            # Apply penalty to these tokens' logits (divide by penalty if logit > 0, multiply if logit < 0)\n",
        "            penalty_mask = jnp.zeros_like(next_token_logits)\n",
        "\n",
        "            # JAX-friendly approach to apply penalty\n",
        "            def update_penalty(carry, token_id):\n",
        "                logits, mask = carry\n",
        "                token_mask = jnp.ones_like(logits) * (jnp.arange(logits.shape[-1]) == token_id)\n",
        "                scaled_logits = jnp.where(\n",
        "                    logits > 0,\n",
        "                    logits / repetition_penalty,\n",
        "                    logits * repetition_penalty\n",
        "                )\n",
        "                new_logits = jnp.where(token_mask, scaled_logits, logits)\n",
        "                new_mask = mask + token_mask\n",
        "                return (new_logits, new_mask), None\n",
        "\n",
        "            (next_token_logits, _), _ = jax.lax.scan(\n",
        "                update_penalty,\n",
        "                (next_token_logits, penalty_mask),\n",
        "                unique_tokens\n",
        "            )\n",
        "\n",
        "        # Apply temperature scaling\n",
        "        if temperature > 0 and temperature != 1.0:\n",
        "            next_token_logits = next_token_logits / temperature\n",
        "\n",
        "        return next_token_logits, key\n",
        "\n",
        "    # Create the sampling function\n",
        "    def sample_token(logits, key):\n",
        "        # Apply top-k filtering\n",
        "        if top_k > 0:\n",
        "            top_k_logits, top_k_indices = jax.lax.top_k(logits, top_k)\n",
        "            # Create a \"impossible\" logit value for tokens not in top-k\n",
        "            min_value = jnp.min(top_k_logits) - 1e10\n",
        "            # Create a mapping from original indices to top-k logits or min_value\n",
        "            logits = jnp.full_like(logits, min_value)\n",
        "            logits = logits.at[top_k_indices].set(top_k_logits)\n",
        "\n",
        "        # Apply top-p (nucleus) filtering\n",
        "        if 0.0 < top_p < 1.0:\n",
        "            # Sort logits in descending order\n",
        "            sorted_logits = jnp.sort(logits, descending=True)\n",
        "            # Calculate cumulative probabilities\n",
        "            sorted_probs = jax.nn.softmax(sorted_logits)\n",
        "            cumulative_probs = jnp.cumsum(sorted_probs)\n",
        "            # Find the threshold where cumulative probability exceeds top_p\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            # Keep the first token that exceeds the threshold\n",
        "            sorted_indices_to_remove = jnp.concatenate([\n",
        "                jnp.zeros(1, dtype=jnp.bool_),\n",
        "                sorted_indices_to_remove[:-1]\n",
        "            ])\n",
        "            # Get the cutoff threshold value\n",
        "            threshold_logit = jnp.min(\n",
        "                jnp.where(sorted_indices_to_remove, sorted_logits, jnp.inf)\n",
        "            )\n",
        "            # Filter logits below the threshold value\n",
        "            logits = jnp.where(logits < threshold_logit, -jnp.inf, logits)\n",
        "\n",
        "        # Sample from the filtered distribution\n",
        "        if do_sample:\n",
        "            key, subkey = jax.random.split(key)\n",
        "            next_token = jax.random.categorical(subkey, logits, axis=-1)\n",
        "        else:\n",
        "            # Greedy decoding - take the argmax\n",
        "            next_token = jnp.argmax(logits, axis=-1)\n",
        "\n",
        "        return next_token, key\n",
        "\n",
        "    # Generate tokens one by one\n",
        "    generated = input_ids.copy()[0]\n",
        "    for _ in range(max_length - len(input_ids[0])):\n",
        "        # Get next token logits\n",
        "        next_token_logits, key = get_next_token_logits(generated[None, :], key)\n",
        "\n",
        "        # Sample the next token\n",
        "        next_token, key = sample_token(next_token_logits[0], key)\n",
        "\n",
        "        # Add the token to our generated sequence\n",
        "        generated = np.append(generated, next_token)\n",
        "\n",
        "        # Check if we've generated an EOS token\n",
        "        if next_token == eos_token_id:\n",
        "            break\n",
        "\n",
        "    # Decode the generated ids back to text\n",
        "    generated_text = tokenizer.decode(generated, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "MKZvJAlxbS3c"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(model,GPT2Tokenizer.from_pretrained(\"gpt2\"),\"Hi my name is\",max_length=35))"
      ],
      "metadata": {
        "id": "mzXzh4oZncGP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "364e3b09-be3b-49b2-80bd-e4fc859c8d76"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi my name is David. I'm a freelance writer and the owner of The Bookstore, where you can find books by authors like Robert Heinlein or Thomas Pynch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XtWa8Ohdn8uM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "992dae584a224f1a9678e1dd7c723753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_999e4ed9a43842ce8e8bc6ed1cf0afa6",
              "IPY_MODEL_c01281c6cb0043bfba6227061a7a7d10",
              "IPY_MODEL_7f80926b1b854f32be61d6c0087b2e8c"
            ],
            "layout": "IPY_MODEL_caa08e69fcf049c0a55b8ff040c9d2ca"
          }
        },
        "999e4ed9a43842ce8e8bc6ed1cf0afa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c75f1c301f5441c8b20dcfbc53c94f6a",
            "placeholder": "​",
            "style": "IPY_MODEL_350f3d47b8af478e96aaaed4645e3738",
            "value": "config.json: 100%"
          }
        },
        "c01281c6cb0043bfba6227061a7a7d10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78b60bea0eb845afad387897c8653ee4",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d792b4752da47ba9cc5d6c80b8c6ead",
            "value": 665
          }
        },
        "7f80926b1b854f32be61d6c0087b2e8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fb9ef51669f4a88871a3e1fe8af506d",
            "placeholder": "​",
            "style": "IPY_MODEL_a0ec79f3fa9b4fa487350a5903446c20",
            "value": " 665/665 [00:00&lt;00:00, 85.3kB/s]"
          }
        },
        "caa08e69fcf049c0a55b8ff040c9d2ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c75f1c301f5441c8b20dcfbc53c94f6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "350f3d47b8af478e96aaaed4645e3738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78b60bea0eb845afad387897c8653ee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d792b4752da47ba9cc5d6c80b8c6ead": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3fb9ef51669f4a88871a3e1fe8af506d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0ec79f3fa9b4fa487350a5903446c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4dcba774df3948f1a2d480088f97a864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_408d80a0ef8f492597d8a5c8082607db",
              "IPY_MODEL_8688e0ec5ef9490d81eb551715d3d5d8",
              "IPY_MODEL_59adf9c199c94095a6db177906b9dd6b"
            ],
            "layout": "IPY_MODEL_3cd3c62e9fb94a0ba659a874fee530f2"
          }
        },
        "408d80a0ef8f492597d8a5c8082607db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3ba0b14c8bc4cd7a159c59685aa478b",
            "placeholder": "​",
            "style": "IPY_MODEL_e8246608c2714657a610fea8ecc7d7f0",
            "value": "model.safetensors: 100%"
          }
        },
        "8688e0ec5ef9490d81eb551715d3d5d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35a0ddefcad74cc2bd7ebd38179d1556",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0a10a02c3a94b94bf41ab39c9ec4573",
            "value": 548105171
          }
        },
        "59adf9c199c94095a6db177906b9dd6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e91d1bf3d70d4f0c833ea6e623d7cf90",
            "placeholder": "​",
            "style": "IPY_MODEL_7459739f444a4af09c663b167d1188f2",
            "value": " 548M/548M [00:02&lt;00:00, 261MB/s]"
          }
        },
        "3cd3c62e9fb94a0ba659a874fee530f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3ba0b14c8bc4cd7a159c59685aa478b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8246608c2714657a610fea8ecc7d7f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35a0ddefcad74cc2bd7ebd38179d1556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0a10a02c3a94b94bf41ab39c9ec4573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e91d1bf3d70d4f0c833ea6e623d7cf90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7459739f444a4af09c663b167d1188f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c38b0b4cebc74037856858e41cc642b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_697bc29630c442f5a735ae25a4199524",
              "IPY_MODEL_d74d190c7a5145ce963814cad271ce42",
              "IPY_MODEL_8e21a1536abe42a780fc49009100bf96"
            ],
            "layout": "IPY_MODEL_226621c79d5d4ff5aa0fcfdae39d0351"
          }
        },
        "697bc29630c442f5a735ae25a4199524": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d381688dd114e79860393ed0c92f1f5",
            "placeholder": "​",
            "style": "IPY_MODEL_354901bed384451983429681b9891c01",
            "value": "generation_config.json: 100%"
          }
        },
        "d74d190c7a5145ce963814cad271ce42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb844b459f82432889cb0aed03eceef8",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba07e34dd1d34bd8bc368e7101d9b7de",
            "value": 124
          }
        },
        "8e21a1536abe42a780fc49009100bf96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df6d571b2a234a328778ca95cf4a95e7",
            "placeholder": "​",
            "style": "IPY_MODEL_b697278b59e94b70b7fe74335483ab9d",
            "value": " 124/124 [00:00&lt;00:00, 16.9kB/s]"
          }
        },
        "226621c79d5d4ff5aa0fcfdae39d0351": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d381688dd114e79860393ed0c92f1f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "354901bed384451983429681b9891c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb844b459f82432889cb0aed03eceef8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba07e34dd1d34bd8bc368e7101d9b7de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df6d571b2a234a328778ca95cf4a95e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b697278b59e94b70b7fe74335483ab9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}